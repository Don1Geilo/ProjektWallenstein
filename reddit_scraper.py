import os
import json
import time
from config import CLIENT_ID, CLIENT_SECRET, USER_AGENT
from datetime import datetime, timezone, timedelta
import praw
from stock_keywords import global_synonyms  # Aktien-Synonyme importieren

# ğŸ”¥ Subreddits, die gescraped werden
SUBREDDITS = ["WallStreetBets", "WallstreetbetsGer", "Mauerstrassenwetten"]

# ğŸ”¥ Speicherpfad fÃ¼r die Reddit-Daten
REDDIT_DATA_FILE = "reddit_data.json"

# ğŸ”¥ Initialisiere die Reddit API
reddit = praw.Reddit(
    client_id=CLIENT_ID,
    client_secret=CLIENT_SECRET,
    user_agent=USER_AGENT
)

# âœ… Debugging der API-Verbindung
try:
    print(f"âœ… Erfolgreich eingeloggt als: {reddit.user.me()}")
except Exception as e:
    print(f"âŒ Fehler beim Verbinden mit Reddit: {e}")


# ğŸ”¥ Funktion zum Scrapen der Posts

def get_reddit_posts_with_comments(subreddit, post_limit=100, comment_limit=25):
    print(f"ğŸ“¥ Scraping {post_limit} Posts von r/{subreddit}...")

    posts = []
    try:
        for submission in reddit.subreddit(subreddit).new(limit=post_limit):
            post_time = submission.created_utc
            dt_hour = datetime.fromtimestamp(post_time, timezone.utc).replace(minute=0, second=0)

            # Kommentare scrapen
            submission.comments.replace_more(limit=0)  # Alle Kommentare laden
            comments = [comment.body for comment in submission.comments[:comment_limit]]

            full_text = (submission.title or "") + " " + (submission.selftext or "") + " " + " ".join(comments)
            
            # ğŸ”¥ PrÃ¼fen, ob relevante Keywords vorhanden sind
            if any(alias.lower() in full_text.lower() for stock in global_synonyms for alias in global_synonyms[stock]):
                post = {
                    "date": post_time,
                    "title": submission.title,
                    "text": submission.selftext,
                    "comments": comments
                }
                posts.append(post)
    
    except Exception as e:
        print(f"âš ï¸ Fehler beim Scrapen von r/{subreddit}: {e}")

    print(f"âœ… {len(posts)} relevante Posts von r/{subreddit} geladen.")
    return posts


# ğŸ”¥ Lade vorhandene Reddit-Daten
def load_reddit_data():
    if os.path.exists(REDDIT_DATA_FILE):
        with open(REDDIT_DATA_FILE, "r", encoding="utf-8") as f:
            try:
                return json.load(f)
            except json.JSONDecodeError:
                print("âš ï¸ Fehler beim Laden von reddit_data.json â€“ Datei wird zurÃ¼ckgesetzt.")
                return []
    return []


# ğŸ”¥ Speichere Reddit-Daten
def save_reddit_data(data):
    # ğŸ”¥ JSON-konforme Umwandlung (z. B. `nan` entfernen)
    for post in data:
        if "date" in post and isinstance(post["date"], (int, float)):
            post["date"] = float(post["date"])  # Sicherheitshalber in Float konvertieren
        else:
            print(f"âš ï¸ Fehlerhafter Post gefunden: {post}")

    with open(REDDIT_DATA_FILE, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=4)


# ğŸ”¥ ÃœberprÃ¼fe und aktualisiere die Reddit-Daten
def update_reddit_data():
    all_posts = load_reddit_data()

    # ğŸ”¥ LÃ¶sche alte Daten (Ã¤lter als 7 Tage)
    seven_days_ago = datetime.now(timezone.utc) - timedelta(days=7)
    filtered_posts = [post for post in all_posts if datetime.fromtimestamp(post["date"], timezone.utc) >= seven_days_ago]
    
    print(f"ğŸ—‘ï¸ Alte Posts entfernt: {len(all_posts) - len(filtered_posts)}")

    # ğŸ”¥ Scrap neue Daten
    new_posts = []
    for sr in SUBREDDITS:
        new_posts.extend(get_reddit_posts_with_comments(sr))

    # ğŸ”¥ Alle Daten zusammenfÃ¼hren
    filtered_posts.extend(new_posts)

    # ğŸ”¥ Speichern
    save_reddit_data(filtered_posts)
    print(f"âœ… Gesamtanzahl der gespeicherten relevanten Posts: {len(filtered_posts)}")


# ğŸ”¥ Starte das Update
if __name__ == "__main__":
    update_reddit_data()
